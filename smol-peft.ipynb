{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807808a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 18:03:17.080790: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-26 18:03:27.389539: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-26 18:03:54.091590: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA GPU: NVIDIA L40S\n",
      "GPU memory: 47.7GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24ccb4bc0ba74ad28e94dfd79dab376c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "from typing import Optional, Dict, Any\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"Using CUDA GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Using Apple MPS\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Using CPU - you will need to use a GPU to train models\")\n",
    "\n",
    "# Authenticate with Hugging Face (optional, for private models)\n",
    "from huggingface_hub import login\n",
    "login()  # Uncomment if you need to access private models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6e9076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both base and instruct models for comparison\n",
    "base_model_name = \"HuggingFaceTB/SmolLM3-3B-Base\"\n",
    "instruct_model_name = \"HuggingFaceTB/SmolLM3-3B\"\n",
    "\n",
    "\n",
    "# Load tokenizers\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "instruct_tokenizer = AutoTokenizer.from_pretrained(instruct_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "129be984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SIMPLE_QA ---\n",
      "Complete conversation format:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 26 November 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\n",
      "\n",
      "<|im_start|>user\n",
      "What is machine learning?<|im_end|>\n",
      "\n",
      "\n",
      "With generation prompt:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 26 November 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\n",
      "\n",
      "<|im_start|>user\n",
      "What is machine learning?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- WITH_SYSTEM ---\n",
      "Complete conversation format:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 26 November 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant specialized in explaining technical concepts clearly.\n",
      "\n",
      "<|im_start|>user\n",
      "What is machine learning?<|im_end|>\n",
      "\n",
      "\n",
      "With generation prompt:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 26 November 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant specialized in explaining technical concepts clearly.\n",
      "\n",
      "<|im_start|>user\n",
      "What is machine learning?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- MULTI_TURN ---\n",
      "Complete conversation format:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 26 November 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a math tutor.\n",
      "\n",
      "<|im_start|>user\n",
      "What is calculus?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Calculus is a branch of mathematics that deals with rates of change and accumulation of quantities.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you give me a simple example?<|im_end|>\n",
      "\n",
      "\n",
      "With generation prompt:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 26 November 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a math tutor.\n",
      "\n",
      "<|im_start|>user\n",
      "What is calculus?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Calculus is a branch of mathematics that deals with rates of change and accumulation of quantities.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you give me a simple example?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- REASONING_TASK ---\n",
      "Complete conversation format:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 26 November 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\n",
      "\n",
      "<|im_start|>user\n",
      "Solve step by step: If a train travels 120 miles in 2 hours, what is its average speed?<|im_end|>\n",
      "\n",
      "\n",
      "With generation prompt:\n",
      "<|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 26 November 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face. Your role as an assistant involves thoroughly exploring questions through a systematic thinking process before providing the final precise and accurate solutions. This requires engaging in a comprehensive cycle of analysis, summarizing, exploration, reassessment, reflection, backtracking, and iteration to develop well-considered thinking process. Please structure your response into two main sections: Thought and Solution using the specified format: <think> Thought section </think> Solution section. In the Thought section, detail your reasoning process in steps. Each step should include detailed considerations such as analysing questions, summarizing relevant findings, brainstorming new ideas, verifying the accuracy of the current steps, refining any errors, and revisiting previous steps. In the Solution section, based on various attempts, explorations, and reflections from the Thought section, systematically present the final solution that you deem correct. The Solution section should be logical, accurate, and concise and detail necessary steps needed to reach the conclusion.\n",
      "\n",
      "<|im_start|>user\n",
      "Solve step by step: If a train travels 120 miles in 2 hours, what is its average speed?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create different types of conversations to test\n",
    "conversations = {\n",
    "    \"simple_qa\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "    ],\n",
    "    \n",
    "    \"with_system\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specialized in explaining technical concepts clearly.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
    "    ],\n",
    "    \n",
    "    \"multi_turn\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a math tutor.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is calculus?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Calculus is a branch of mathematics that deals with rates of change and accumulation of quantities.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Can you give me a simple example?\"},\n",
    "    ],\n",
    "    \n",
    "    \"reasoning_task\": [\n",
    "        {\"role\": \"user\", \"content\": \"Solve step by step: If a train travels 120 miles in 2 hours, what is its average speed?\"},\n",
    "    ]\n",
    "}\n",
    "\n",
    "for conv_type, messages in conversations.items():\n",
    "    print(f\"--- {conv_type.upper()} ---\")\n",
    "    \n",
    "    # Format without generation prompt (for completed conversations)\n",
    "    formatted_complete = instruct_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    \n",
    "    # Format with generation prompt (for inference)\n",
    "    formatted_prompt = instruct_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    print(\"Complete conversation format:\")\n",
    "    print(formatted_complete)\n",
    "    print(\"\\nWith generation prompt:\")\n",
    "    print(formatted_prompt)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cc73f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROCESSING GSM8K DATASET ===\n",
      "\n",
      "Original GSM8K example: {'question': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'answer': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'}\n",
      "Processed example: {'messages': [{'content': 'You are a math tutor. Solve problems step by step.', 'role': 'system'}, {'content': 'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?', 'role': 'user'}, {'content': 'Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72', 'role': 'assistant'}]}\n"
     ]
    }
   ],
   "source": [
    "# Function to process different dataset formats\n",
    "def process_qa_dataset(examples, question_col, answer_col):\n",
    "    \"\"\"Process Q&A datasets into chat format\"\"\"\n",
    "    processed = []\n",
    "    \n",
    "    for question, answer in zip(examples[question_col], examples[answer_col]):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "        processed.append(messages)\n",
    "    \n",
    "    return {\"messages\": processed}\n",
    "\n",
    "def process_instruction_dataset(examples):\n",
    "    \"\"\"Process instruction-following datasets\"\"\"\n",
    "    processed = []\n",
    "    \n",
    "    for instruction, response in zip(examples[\"instruction\"], examples[\"response\"]):\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": instruction},\n",
    "            {\"role\": \"assistant\", \"content\": response}\n",
    "        ]\n",
    "        processed.append(messages)\n",
    "    \n",
    "    return {\"messages\": processed}\n",
    "\n",
    "# Example: Process GSM8K math dataset\n",
    "print(\"=== PROCESSING GSM8K DATASET ===\\n\")\n",
    "\n",
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\", split=\"train[:100]\")  # Small subset for demo\n",
    "print(f\"Original GSM8K example: {gsm8k[0]}\")\n",
    "\n",
    "# Convert to chat format\n",
    "def process_gsm8k(examples):\n",
    "    processed = []\n",
    "    for question, answer in zip(examples[\"question\"], examples[\"answer\"]):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a math tutor. Solve problems step by step.\"},\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": answer}\n",
    "        ]\n",
    "        processed.append(messages)\n",
    "    return {\"messages\": processed}\n",
    "\n",
    "gsm8k_processed = gsm8k.map(process_gsm8k, batched=True, remove_columns=gsm8k.column_names)\n",
    "print(f\"Processed example: {gsm8k_processed[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30635854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply chat templates to processed datasets\n",
    "def apply_chat_template_to_dataset(dataset, tokenizer):\n",
    "    \"\"\"Apply chat template to dataset for training\"\"\"\n",
    "    \n",
    "    def format_messages(examples):\n",
    "        formatted_texts = []\n",
    "        \n",
    "        for messages in examples[\"messages\"]:\n",
    "            # Apply chat template\n",
    "            formatted_text = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False  # We want the complete conversation\n",
    "            )\n",
    "            formatted_texts.append(formatted_text)\n",
    "        \n",
    "        return {\"text\": formatted_texts}\n",
    "    \n",
    "    return dataset.map(format_messages, batched=True)\n",
    "\n",
    "# # Apply to our processed GSM8K dataset\n",
    "# gsm8k_formatted = apply_chat_template_to_dataset(gsm8k_processed, instruct_tokenizer)\n",
    "# print(\"=== FORMATTED TRAINING DATA ===\")\n",
    "# print(gsm8k_formatted[0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95eab0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading HuggingFaceTB/SmolLM3-3B-Base...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed25a484b5ae45058d98c4e8f5685cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded! Parameters: 3,075,098,624\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for fine-tuning\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import wandb  # Optional: for experiment tracking\n",
    "\n",
    "# Initialize Weights & Biases (optional)\n",
    "# wandb.init(project=\"smollm3-finetuning\")\n",
    "\n",
    "# Load SmolLM3 base model for fine-tuning\n",
    "model_name = \"HuggingFaceTB/SmolLM3-3B-Base\"\n",
    "new_model_name = \"SmolLM3-Custom-SFT\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "tokenizer.padding_side = \"right\"  # Padding on the right for generation\n",
    "\n",
    "print(f\"Model loaded! Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c04a600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPARING DATASET ===\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8b1fa967d984ef6ae2c70561cd59dc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c801a676b034921b416058112af2557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa02294135bc4e21a9db5bbdded689b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/113 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceaf6e1ea99c41bb9ce93293a2716248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 1000\n",
      "Example: {'messages': [{'content': 'Hi there', 'role': 'user'}, {'content': '<think>\\nOkay, the user sent \"Hi there\". That\\'s a friendly greeting. I should respond in a welcoming way. Let me check the guidelines. I need to be helpful, keep the conversation going, and maybe ask how I can assist them. Let me make sure the tone is warm and approachable. Alright, something like \"Hello! How can I assist you today?\" That should work. Let me confirm there\\'s no typo and it\\'s in a natural, conversational style.\\n</think>\\n\\nHello! How can I assist you today?', 'role': 'assistant'}, {'content': \"I'm looking for a healthy breakfast idea. What's a good option?\", 'role': 'user'}, {'content': \"<think>\\nOkay, the user is asking for a healthy breakfast idea. Let me think about what makes a breakfast healthy. It should be balanced, providing a mix of nutrients like protein, fiber, healthy fats, and some carbs. Let me brainstorm some options.\\n\\nMaybe start with a classic like avocado toast. It's popular and has good fats and fiber. Adding an egg or Greek yogurt could boost the protein. Oh, and maybe some veggies on top for extra nutrients. That could work.\\n\\nAnother option could be a smoothie. Smoothies are quick and can pack a lot of nutrients. Using spinach, banana, berries, and a protein powder or Greek yogurt would be good. Maybe add some nut butter for healthy fats. That's a solid choice too.\\n\\nOatmeal is another staple. Overnight oats are convenient. Mixing oats with milk, chia seeds, and fruits. Maybe add some nuts or seeds for crunch. It's high in fiber and can be customized.\\n\\nWhat about a breakfast bowl with quinoa or another grain? Topped with fruits, nuts, and a drizzle of honey or nut butter. That's another balanced option.\\n\\nI should also consider dietary restrictions. Maybe mention alternatives for those who are vegan, gluten-free, etc. For example, using almond milk instead of dairy in the smoothie or choosing gluten-free oats.\\n\\nPortion sizes and preparation time might be important too. The user might be looking for something quick or something they can prep ahead. The avocado toast is quick, the overnight oats can be prepped the night before.\\n\\nIncluding a variety of options gives the user choices based on their preferences and time. Let me structure the response with a couple of options, each with ingredients and preparation steps. Also, highlight the nutritional benefits of each. Make sure to keep the tone friendly and helpful.\\n</think>\\n\\nHereâ€™s a quick and nutritious breakfast idea thatâ€™s easy to customize:\\n\\n### **Avocado Toast with a Twist**  \\n**Ingredients:**  \\n- 1 slice whole-grain or sourdough bread (toasted)  \\n- Â½ ripe avocado, mashed  \\n- 1 poached or scrambled egg (or a dollop of Greek yogurt for a vegan option)  \\n- Sprinkle of chili flakes, black pepper, and a pinch of sea salt  \\n- Optional toppings: cherry tomatoes, microgreens, or a drizzle of olive oil  \\n\\n**Why itâ€™s healthy:**  \\n- **Whole grains** provide fiber for sustained energy.  \\n- **Avocado** adds healthy fats and potassium.  \\n- **Eggs/Greek yogurt** offer high-quality protein to keep you full.  \\n- **Spices** add flavor without extra calories.  \\n\\n---\\n\\n### **Protein-Packed Smoothie Bowl**  \\n**Ingredients:**  \\n- 1 frozen banana  \\n- Â½ cup frozen berries (blueberries, strawberries, etc.)  \\n- Â½ cup unsweetened almond milk (or any milk of choice)  \\n- 1 scoop vanilla protein powder (or Â¼ cup Greek yogurt)  \\n- 1 tbsp chia seeds or flaxseeds  \\n- Toppings: sliced fruit, granola, nuts, or cacao nibs  \\n\\n**Blend it up**, pour into a bowl, and add toppings.  \\n\\n**Why itâ€™s healthy:**  \\n- **Fiber and antioxidants** from fruits and seeds.  \\n- **Protein** supports muscle repair and satiety.  \\n- **Healthy fats** from seeds or nut toppings.  \\n\\n---\\n\\n### **Overnight Oats (Make-Ahead Option)**  \\n**Ingredients:**  \\n- Â½ cup rolled oats  \\n- Â½ cup milk (dairy or plant-based)  \\n- 1 tbsp nut butter (peanut, almond, etc.)  \\n- 1 tsp honey or maple syrup  \\n- Â½ tsp chia seeds  \\n- Toppings: fresh fruit, cinnamon, or a sprinkle of granola  \\n\\n**Instructions:**  \\nMix all ingredients in a jar, refrigerate overnight, and enjoy cold or at room temperature.  \\n\\n**Why itâ€™s healthy:**  \\n- **Slow-digesting carbs** for steady energy.  \\n- **Chia seeds** add omega-3s and fiber.  \\n- **Nut butter** provides protein and healthy fats.  \\n\\n---\\n\\nLet me know if youâ€™d like more ideas or adjustments for dietary needs (e.g., vegan, gluten-free, low-carb)! ðŸ˜Š\", 'role': 'assistant'}], 'chat_template_kwargs': {'custom_instructions': '', 'enable_thinking': True, 'python_tools': [], 'xml_tools': []}, 'source': 'smoltalk-everyday-convs-reasoning-Qwen3-32B'}\n",
      "Formatted example: <|im_start|>system\n",
      "## Metadata\n",
      "\n",
      "Knowledge Cutoff Date: June 2025\n",
      "Today Date: 26 November 2025\n",
      "Reasoning Mode: /think\n",
      "\n",
      "## Custom Instructions\n",
      "\n",
      "You are a helpful AI assistant named SmolLM, trained by Hu...\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare training dataset\n",
    "print(\"=== PREPARING DATASET ===\\n\")\n",
    "\n",
    "# Option 1: Use SmolTalk2 (recommended for beginners)\n",
    "dataset = load_dataset(\"HuggingFaceTB/smoltalk2\", \"SFT\")\n",
    "train_dataset = dataset[\"smoltalk_everyday_convs_reasoning_Qwen3_32B_think\"].select(range(1000))  # Use subset for faster training\n",
    "\n",
    "# Option 2: Use your own processed dataset from Exercise 2\n",
    "# train_dataset = gsm8k_formatted.select(range(500))\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Example: {train_dataset[0]}\")\n",
    "\n",
    "# Prepare the dataset for SFT\n",
    "def format_chat_template(example):\n",
    "    \"\"\"Format the messages using the chat template\"\"\"\n",
    "    if \"messages\" in example:\n",
    "        # SmolTalk2 format\n",
    "        messages = example[\"messages\"]\n",
    "    else:\n",
    "        # Custom format - adapt as needed\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"response\"]}\n",
    "        ]\n",
    "    \n",
    "    # Apply chat template\n",
    "    text = instruct_tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False,\n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = train_dataset.map(format_chat_template)\n",
    "formatted_dataset = formatted_dataset.remove_columns(\n",
    "    [col for col in formatted_dataset.column_names if col != \"text\"]\n",
    ")\n",
    "print(f\"Formatted example: {formatted_dataset[0]['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dcf32ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration set!\n",
      "Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "# Configure training parameters\n",
    "training_config = SFTConfig(\n",
    "    # Model and data\n",
    "    output_dir=f\"./{new_model_name}\",\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=2048,\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    per_device_train_batch_size=4,  # Adjust based on your GPU memory\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=1,  # Start with 1 epoch\n",
    "    max_steps=500,  # Limit steps for demo\n",
    "    \n",
    "    # Optimization\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adamw_torch\",\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,\n",
    "    \n",
    "    # Memory optimization\n",
    "    dataloader_num_workers=0,\n",
    "    group_by_length=True,  # Group similar length sequences\n",
    "    \n",
    "    # Hugging Face Hub integration\n",
    "    push_to_hub=True,  # Set to True to upload to Hub\n",
    "    hub_model_id=f\"dingusagar/{new_model_name}\",\n",
    "    \n",
    "    # Experiment tracking\n",
    "    report_to=[\"wandb\"],  # Use wandb for experiment tracking\n",
    "    run_name=f\"{new_model_name}-training\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration set!\")\n",
    "print(f\"Effective batch size: {training_config.per_device_train_batch_size * training_config.gradient_accumulation_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9ebc516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': None}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA trainingâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdingusagar\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/storage/ice1/6/8/dk305/llm-finetuning-course/wandb/run-20251126_181336-vsbktr6z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dingusagar/huggingface/runs/vsbktr6z' target=\"_blank\">SmolLM3-Custom-SFT-training</a></strong> to <a href='https://wandb.ai/dingusagar/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dingusagar/huggingface' target=\"_blank\">https://wandb.ai/dingusagar/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dingusagar/huggingface/runs/vsbktr6z' target=\"_blank\">https://wandb.ai/dingusagar/huggingface/runs/vsbktr6z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 42:39, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.573300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.658000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.478800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.381100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.224600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.944700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.890200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.838600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.867100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.830400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.820100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.800400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.798400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.837700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.771200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.848600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.781900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.816100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.784500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.753600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.824200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.757800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.780100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.799000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.772000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.791500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.772700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.753100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.818600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.753800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.732400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.796500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.751700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.783700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.772800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.814800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.738000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.749200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.795300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.754500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.754200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.8971047964096069, metrics={'train_runtime': 2574.3177, 'train_samples_per_second': 3.108, 'train_steps_per_second': 0.194, 'total_flos': 2.0406916190571725e+17, 'train_loss': 0.8971047964096069, 'epoch': 7.944})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LoRA configuration with PEFT\n",
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],   # REQUIRED\n",
    "\n",
    ")\n",
    "\n",
    "# Create SFTTrainer with LoRA enabled\n",
    "from trl import SFTTrainer\n",
    "\n",
    "lora_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,  # dataset with a \"text\" field or messages + dataset_text_field in config\n",
    "    args=training_config,\n",
    "    peft_config=peft_config,  # << enable LoRA\n",
    ")\n",
    "\n",
    "print(\"Starting LoRA trainingâ€¦\")\n",
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6331ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
